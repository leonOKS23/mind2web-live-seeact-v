"""Script to run end-to-end evaluation on the benchmark.

Modified from https://github.com/web-arena-x/webarena/blob/main/run.py.
"""
import argparse
import collections
import copy
import glob
import heapq
import json
import logging
import os
import random
import subprocess
import tempfile
import time
import pdb
from pathlib import Path
from typing import List

# Default local vLLM/OpenAI-compatible config (can be overridden by env vars)
os.environ.setdefault("DATASET", "webcanvas")
os.environ.setdefault("OPENAI_API_BASE", "http://127.0.0.1:8000/v1")
os.environ.setdefault("OPENAI_API_KEY", "EMPTY")
os.environ.setdefault("PROCESSOR_SOURCE", "my_processors")
os.environ.setdefault("GROUNDING_OPENAI_API_BASE", "http://127.0.0.1:8001/v1")

import openai
import requests
import torch
from PIL import Image
import sys

#sys.path.append('/media/czj/GT/search_agent')
from agent import (
    PromptAgent,
    construct_agent,
)
from browser_env.utils import *
from agent.prompts import *
from browser_env import (
    Action,
    ActionTypes,
    ScriptBrowserEnv,
    StateInfo,
    Trajectory,
    create_stop_action,
)
from browser_env.actions import is_equivalent, create_goto_url_action, _id2key
from browser_env.auto_login import get_site_comb_from_filepath
from browser_env.helper_functions import (
    RenderHelper,
    get_action_description,
)
from evaluation_harness import evaluator_router, image_utils, evaluate_with_webcanvas
from evaluation_harness.webcanvas_evaluators import parse_reference_evaluation
## passk parallel
try:
    from scripts.multi_docker_urls import MULTI_URL_MAPs
except Exception:
    MULTI_URL_MAPs = {}
import multiprocessing as mp
import re

##

DATASET = os.environ["DATASET"]

import logging
import time
import random
from pathlib import Path
import pdb

LOG_FOLDER = "log_files"
Path(LOG_FOLDER).mkdir(parents=True, exist_ok=True)
LOG_FILE_NAME = f"{LOG_FOLDER}/log_{time.strftime('%Y%m%d%H%M%S', time.localtime())}_{random.randint(0, 10000)}.log"

logger = logging.getLogger("logger")
logger.setLevel(logging.INFO)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
logger.addHandler(console_handler)

file_handler = logging.FileHandler(LOG_FILE_NAME)
file_handler.setLevel(logging.DEBUG)
logger.addHandler(file_handler)

# Set the log format to include filename and line number
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s")
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

RENDER = True

def config() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Run end-to-end evaluation on the benchmark"
    )
    parser.add_argument(
        "--render", action="store_true", help="Render the browser"
    )

    parser.add_argument(
        "--slow_mo",
        type=int,
        default=0,
        help="Slow down the browser by the specified amount",
    )
    parser.add_argument(
        "--action_set_tag", default="id_accessibility_tree", help="Action type"
    )
    parser.add_argument(
        "--observation_type",
        choices=[
            "accessibility_tree",
            "accessibility_tree_with_captioner",
            "html",
            "image",
            "image_som",
        ],
        default="accessibility_tree",
        help="Observation type",
    )
    parser.add_argument(
        "--current_viewport_only",
        action="store_true",
        help="Only use the current viewport for the observation",
    )
    parser.add_argument('--task_ids', type=int, nargs='+',
                        help='an integer for the list to be processed')
    parser.add_argument(
        "--batch_tasks_file",
        type=str,
        default=None,
        help="Path to a batch tasks JSON file (e.g., mind2web-live_test_20241024.json).",
    )
    parser.add_argument("--viewport_width", type=int, default=1280)
    parser.add_argument("--viewport_height", type=int, default=2048)
    parser.add_argument("--save_trace_enabled", action="store_true")
    parser.add_argument("--sleep_after_execution", type=float, default=0.0)

    parser.add_argument("--max_steps", type=int, default=30)

    # agent config
    parser.add_argument("--agent_type", type=str, default="prompt", choices=["prompt", "search"])
    parser.add_argument(
        "--instruction_path",
        type=str,
        default="agents/prompts/state_action_agent.json",
    )
    parser.add_argument(
        "--grounding_method",
        type=str,
        default="uground",
        choices=["uground", "uground_script", "qwen3_vl", "uground_vllm", "simple", "atlas"],
        help="Grounding method used for coordinate-based actions.",
    )
    parser.add_argument(
        "--grounding_model",
        type=str,
        default=None,
        help="Model name for grounding (defaults to --model if not set).",
    )
    parser.add_argument(
        "--grounding_api_base",
        type=str,
        default=None,
        help="OpenAI-compatible base URL for grounding (defaults to OPENAI_API_BASE).",
    )
    parser.add_argument(
        "--uground_script_path",
        type=str,
        default=None,
        help="Path to uground_qwen3vl.py (for grounding_method=uground_script).",
    )
    parser.add_argument(
        "--uground_script_python",
        type=str,
        default=None,
        help="Python executable for uground_script (defaults to python).",
    )
    parser.add_argument(
        "--parsing_failure_th",
        help="When consecutive parsing failures exceed this threshold, the agent will terminate early.",
        type=int,
        default=3,
    )
    parser.add_argument(
        "--repeating_action_failure_th",
        help="When consecutive repeated actions exceed this threshold, the agent will terminate early.",
        type=int,
        default=5,
    )

    parser.add_argument("--test_config_base_dir", type=str)

    parser.add_argument(
        "--eval_captioning_model_device",
        type=str,
        default="cuda",
        choices=["cpu", "cuda"],
        help="Device to run eval captioning model on. By default, runs it on CPU.",
    )
    parser.add_argument(
        "--eval_captioning_model",
        type=str,
        default="none",
        help="Captioning backbone for VQA-type evals.",
    )
    parser.add_argument(
        "--captioning_model",
        type=str,
        default="none",
        help="Captioning backbone for accessibility tree alt text.",
    )

    # lm config
    parser.add_argument("--provider", type=str, default="openai")
    parser.add_argument("--model", type=str, default="gpt-3.5-turbo-0613")
    parser.add_argument("--mode", type=str, default="chat")
    parser.add_argument("--temperature", type=float, default=1.0)
    parser.add_argument("--top_p", type=float, default=0.9)
    parser.add_argument("--context_length", type=int, default=0)
    parser.add_argument("--max_tokens", type=int, default=384)
    parser.add_argument("--cap_gpu", type=int, default=1)
    parser.add_argument("--stop_token", type=str, default="")
    parser.add_argument(
        "--max_retry",
        type=int,
        help="max retry times to perform generations when parsing fails",
        default=1,
    )
    parser.add_argument(
        "--max_obs_length",
        type=int,
        help="when not zero, will truncate the observation to this length before feeding to the model",
        default=3840,
    )

    parser.add_argument(
        "--current_port",
        type=str,
        help="The port id of current cookies",
        default="9981_7771",
    )
    # search config
    parser.add_argument("--max_depth", type=int, default=4, help="Max depth for search agents.")
    parser.add_argument("--branching_factor", type=int, default=5,
                        help="Branching factor at each step for the search agent.")
    parser.add_argument("--search_algo", type=str, default="vf", help="Search algorithm to use",
                        choices=["vf", "bfs", "dfs"])
    parser.add_argument("--vf_budget", type=int, default=20,
                        help="Budget for the number of value function evaluations.")
    parser.add_argument("--value_function", type=str, default="gpt-4o", help="What value function to use.")

    # example config
    parser.add_argument("--test_idx", type=str, default=None, help="Idx to test")
    parser.add_argument("--test_start_idx", type=int, default=0)
    parser.add_argument("--test_end_idx", type=int, default=910)

    # logging related
    parser.add_argument("--result_dir", type=str, default="")

    # passk
    parser.add_argument('--num_processes', type=int, default=4)
    args = parser.parse_args()

    # check the whether the action space is compatible with the observation space
    if (
            args.action_set_tag == "id_accessibility_tree"
            and args.observation_type
            not in [
        "accessibility_tree",
        "accessibility_tree_with_captioner",
        "image_som",
    ]
    ):
        raise ValueError(
            f"Action type {args.action_set_tag} is incompatible with the observation type {args.observation_type}"
        )

    return args


def early_stop(
        trajectory: Trajectory, max_steps: int, thresholds: dict[str, int]
) -> tuple[bool, str]:
    """Check whether need to stop early"""

    # reach the max step

    num_steps = (len(trajectory) - 1) / 2
    print("num_steps", num_steps)
    if num_steps >= max_steps:
        return True, f"Reach max steps {max_steps}"

    last_k_actions: list[Action]
    action_seq: list[Action]

    # Case: parsing failure for k times
    k = thresholds["parsing_failure"]
    last_k_actions = trajectory[1::2][-k:]  # type: ignore[assignment]
    if len(last_k_actions) >= k:
        if all(
                [
                    action["action_type"] == ActionTypes.NONE
                    for action in last_k_actions
                ]
        ):
            return True, f"Failed to parse actions for {k} times"

    # Case: same action for k times
    k = thresholds["repeating_action"]
    last_k_actions = trajectory[1::2][-k:]  # type: ignore[assignment]
    action_seq = trajectory[1::2]  # type: ignore[assignment]

    if len(action_seq) == 0:
        return False, ""

    last_action: Action = action_seq[-1]

    if last_action["action_type"] != ActionTypes.TYPE:
        if len(last_k_actions) >= k:
            if all(
                    [
                        is_equivalent(action, last_action)
                        for action in last_k_actions
                    ]
            ):
                return True, f"Same action for {k} times"

    else:
        # check the action sequence
        if (
                sum([is_equivalent(action, last_action) for action in action_seq])
                >= k
        ):
            return True, f"Same typing action for {k} times"

    return False, ""


def update_passk_log(result_dir, res_record):
    # Read the current content of the log file
    if os.path.exists(f"{result_dir}/result.txt"):
        with open(f"{result_dir}/result.txt", 'r', encoding='utf-8') as file:
            lines = file.readlines()
    else:
        lines = []

    # Filter out the lines that contain the current test file name
    lines = [line for line in lines if res_record["config_file"] not in line]

    # Add the current test file name to the end of the list
    record_str = f"[NAME] {res_record['config_file']} - [INTENT] {res_record['intent']}"
    if 'ERROR' in res_record.keys():
        record_str = record_str.strip()
        record_str += f"[ERROR] {res_record['ERROR']}\n"
    else:
        record_str += f"{res_record['result']}\n"
    lines.append(record_str)

    # Write the updated list back to the log file
    with open(f"{result_dir}/result.txt", 'w', encoding='utf-8') as file:
        file.writelines(lines)


def update_test_log(result_dir, res_record, rank):
    # Read the current content of the log file
    if os.path.exists(f"{result_dir}/result_{rank}.txt"):
        with open(f"{result_dir}/result_{rank}.txt", 'r', encoding='utf-8') as file:
            lines = file.readlines()
    else:
        lines = []

    # Filter out the lines that contain the current test file name
    lines = [line for line in lines if res_record["config_file"] not in line]

    # Add the current test file name to the end of the list
    record_str = f"[NAME] {res_record['config_file']} - [INTENT] {res_record['intent']}"
    if 'ERROR' in res_record.keys():
        record_str = record_str.strip()
        record_str += f"[ERROR] {res_record['ERROR']}\n"
    else:
        record_str += f"{res_record['result']}\n"
    lines.append(record_str)

    # Write the updated list back to the log file
    with open(f"{result_dir}/result_{rank}.txt", 'w', encoding='utf-8') as file:
        file.writelines(lines)


def meta_test(args, config_file_list):
    # set_env_variable
    '''
    if DATASET == 'visualwebarena':
        websites = list(MULTI_URL_MAPs.keys())
        my_urls = {k: MULTI_URL_MAPs[k][0 % len(MULTI_URL_MAPs[k])] for k in websites}
        for k in websites:
            os.environ[k] = my_urls[k]
    elif DATASET == 'webarena':
        pass  # currently not implemented for webarena
    # Regenerate test_data for this process at the result dir
    output_dir = os.path.join(args.result_dir, f"test_data_rank{0}")
    os.makedirs(output_dir, exist_ok=True)
    os.system(f"python scripts/generate_test_data.py {output_dir} {args.result_dir.split('/')[-1]}")
    pdb.set_trace()
    # rename config_file_list
    new_config_list = []
    for config_file_path in config_file_list:
        file_name = config_file_path.split('/')[-1]
        new_config_list.append(os.path.join(output_dir, file_name))
    '''
    test(args, config_file_list)
def test(
        args: argparse.Namespace,
        config_file_list: list[str],

) -> None:
    scores = []
    max_steps = args.max_steps
    branching_factor = args.branching_factor
    assert args.vf_budget is not None, "Value function budget should be specified."

    early_stop_thresholds = {
        "parsing_failure": args.parsing_failure_th,
        "repeating_action": args.repeating_action_failure_th,
    }
    if args.observation_type in [
        "accessibility_tree_with_captioner",
        "image_som",
    ]:
        device = torch.device(f"cuda:{str(args.cap_gpu)}") if torch.cuda.is_available() else "cpu"
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        caption_image_fn = image_utils.get_captioning_fn(
            device, dtype, args.captioning_model
        )
    else:
        caption_image_fn = None
    # Load a (possibly different) captioning model for running VQA evals.

    if (
            caption_image_fn
            and args.eval_captioning_model == args.captioning_model
    ):
        eval_caption_image_fn = caption_image_fn
    else:
        eval_caption_image_fn = image_utils.get_captioning_fn(
            f"cuda:{str(args.cap_gpu)}",
            torch.float16
            if (
                    torch.cuda.is_available()
                    and args.eval_captioning_model_device == "cuda"
            )
            else torch.float32,
            args.eval_captioning_model,
        )

    agent = construct_agent(
        args,
        captioning_fn=None,
    )  # NOTE: captioning_fn here is used for captioning input images.

    env = ScriptBrowserEnv(
        headless=True,
        slow_mo=args.slow_mo,
        observation_type=args.observation_type,
        current_viewport_only=args.current_viewport_only,
        viewport_size={
            "width": args.viewport_width,
            "height": args.viewport_height,
        },
        save_trace_enabled=args.save_trace_enabled,
        sleep_after_execution=args.sleep_after_execution,
        # NOTE: captioning_fn here is used for LLM + captioning baselines.
        # This can be different from the captioning model used for evals.
        captioning_fn=caption_image_fn,
    )

    for config_num, config_file in enumerate(config_file_list):
        if RENDER:
            render_helper = RenderHelper(
                config_file, args.result_dir, args.action_set_tag
            )

        res_record = {"config_file": config_file}
        try:
            # Load task.
            with open(config_file) as f:
                _c = json.load(f)
                intent = _c["intent"]
                task_id = _c["task_id"]
                image_paths = _c.get("image", None)
                images = []
                ### We have generate all the required cookies in advance, so we do not need to generate it here.
                #automatically login
                if "storage_state" in _c and _c["storage_state"]:
                    cookie_file_name = os.path.basename(_c["storage_state"])
                    comb = get_site_comb_from_filepath(cookie_file_name)
                    temp_dir = tempfile.mkdtemp()
                    # subprocess to renew the cookie
                    subprocess.run(
                        [
                            "python",
                            "browser_env/auto_login.py",
                            "--auth_folder",
                            temp_dir,
                            "--site_list",
                            *comb,
                        ]
                    )
                    _c["storage_state"] = f"{temp_dir}/{cookie_file_name}"
                    result = subprocess.run(['pwd'], capture_output=True, text=True)
                    assert os.path.exists(_c["storage_state"])
                    # update the config file
                    config_file = f"{temp_dir}/{os.path.basename(config_file)}"
                    with open(config_file, "w") as f:
                        json.dump(_c, f)
                try:
                    # Load input images for the task, if any.
                    if image_paths is not None:
                        if isinstance(image_paths, str):
                            image_paths = [image_paths]
                        for image_path in image_paths:
                            input_image = load_image(image_path)
                            images.append(input_image)
                except:
                    pdb.set_trace()
            logger.info(f"[Config file]: {config_file}")
            logger.info(f"[Intent]: {intent}")
            res_record["intent"] = intent
            agent.reset(config_file)
            trajectory: Trajectory = []
            if DATASET == 'webcanvas':
                _, _, reference_task_length, reference_evaluate_steps=parse_reference_evaluation(_c)
                evaluate_steps = reference_evaluate_steps


            obs, info = env.reset(options={"config_file": config_file})
            state_info: StateInfo = {"observation": obs, "info": info, "url": env.page.url}
            trajectory.append(state_info)
            meta_data = {"action_history": ["None"]}

            while True:
                early_stop_flag, stop_info = early_stop(
                    trajectory, max_steps, early_stop_thresholds
                )
                viewport_info = info['viewport_info']

                if early_stop_flag:
                    action = create_stop_action(f"Early stop: {stop_info}")
                else:
                    try:
                        action = agent.next_action(
                            trajectory,
                            intent,
                            images=images,
                            meta_data=meta_data,
                            viewport_info=viewport_info )
                    except ValueError as e:
                        # get the error message
                        raise e
                        action = create_stop_action(f"ERROR: {str(e)}")

                print("ACTION TAKEN:", action)
                pre_actions_str = describe_prev_actions(agent.output_dict_list)
                action["pre_actions_str"] = pre_actions_str

                trajectory.append(action)


                if RENDER:
                    render_helper.render(
                        action, state_info, meta_data, args.render_screenshot)

                meta_data["action_history"].append(action["raw_prediction"])
                if DATASET=='webcanvas':

                    #selector=get_locator_at_position(env.page,action['coords'][0],action['coords'][1])
                    viewport_size = env.page.viewport_size
                    selector=(action['coords'][0]* viewport_size["width"] , action['coords'][1]*viewport_size["height"])
                    action_text="".join([_id2key[key] for key in action['text']])
                    if action_text=="":
                        action_text="None"
                    evaluate_steps, step_score, match_result, task_finished = evaluate_with_webcanvas(page=env.page, selector=selector, target_value=action_text, evaluate_steps=evaluate_steps, reference_evaluate_steps=reference_evaluate_steps)
                    logger.info("evaluate with webcanvas")
                    #logger.info(f"selector: {str(selector)}")
                    logger.info(f"Step score: {step_score}")
                    logger.info(f"Match result: {match_result}")
                    logger.info(f"Task Finished ? : {task_finished}")
                if action["action_type"] == ActionTypes.STOP:
                    stop_trajectory = True
                    break
                obs, _, terminated, _, info = env.step(action)
                state_info = {"observation": obs, "info": info, "url": env.page.url}
                trajectory.append(state_info)
                if DATASET == 'webcanvas':
                    if task_finished:
                        terminated = True

                if terminated:
                    # add a action place holder
                    trajectory.append(create_stop_action(""))
                    stop_trajectory = True
                    break

            # NOTE: eval_caption_image_fn is used for running eval_vqa functions.
            if DATASET == 'webcanvas':
                score = eval(step_score)
            else:
                evaluator = evaluator_router(
                    config_file, captioning_fn=eval_caption_image_fn
                )
                score = evaluator(
                    trajectory=trajectory,
                    config_file=config_file,
                    page=env.page
                )
            scores.append(score)

            if score == 1:
                res_record["result"] = f"[Result] (PASS with step: {step_score} score: {score}) {res_record['config_file']}"
            elif score==0:
                res_record["result"] = f"[Result] (FAIL with step: {step_score} score: {score}) {res_record['config_file']}"
            else:
                res_record["result"] = f"[Result] (HALF with step: {step_score} score: {score}) {res_record['config_file']}"

            if args.save_trace_enabled:
                env.save_trace(
                    Path(args.result_dir) / "traces" / f"{task_id}.zip"
                )
        except openai.OpenAIError as e:
            logger.info(f"[OpenAI Error] {repr(e)}")
            res_record['ERROR'] = f"[OpenAI Error] {repr(e)}"
        except Exception as e:
            logger.info(f"[Unhandled Error] {repr(e)}]")
            res_record['ERROR'] = res_record['ERROR'] = f"[OpenAI Error] {repr(e)}"
            import traceback

            # write to error file
            with open(Path(args.result_dir) / "error.txt", "a") as f:
                f.write(f"[Config file]: {config_file}\n")
                f.write(f"[Unhandled Error] {repr(e)}\n")
                f.write(traceback.format_exc())  # write stack trace to file
        update_test_log(args.result_dir, res_record, 0)
        if RENDER:
            render_helper.close()


    env.close()
    if len(scores):
        logger.info(f"Average score: {sum(scores) / len(scores)} at rank {0}")


def prepare(args: argparse.Namespace) -> None:
    # convert prompt python files to json
    from agent.prompts import to_json

    to_json.run()

    # prepare result dir
    result_dir = args.result_dir
    if not result_dir:
        result_dir = (
            f"cache/results_{time.strftime('%Y%m%d%H%M%S', time.localtime())}"
        )
    if not Path(result_dir).exists():
        Path(result_dir).mkdir(parents=True, exist_ok=True)
        args.result_dir = result_dir
        logger.info(f"Create result dir: {result_dir}")

    if not (Path(result_dir) / "traces").exists():
        (Path(result_dir) / "traces").mkdir(parents=True, exist_ok=True)

    # log the log file
    with open(os.path.join(result_dir, "log_files.txt"), "a+") as f:
        f.write(f"{LOG_FILE_NAME}\n")


def get_unfinished(config_files: list[str], result_dir: str) -> list[str]:
    result_files = glob.glob(f"{result_dir}/*.html")
    print(result_files)
    task_ids = [
        os.path.basename(f).split(".")[0].split("_")[1] for f in result_files
    ]
    print(task_ids)
    unfinished_configs = []
    for config_file in config_files:
        task_id = os.path.basename(config_file).split(".")[0]
        if task_id not in task_ids:
            unfinished_configs.append(config_file)
    return unfinished_configs


def dump_config(args: argparse.Namespace) -> None:
    config_file = Path(args.result_dir) / "config.json"
    if not config_file.exists():
        with open(config_file, "w") as f:
            json.dump(vars(args), f, indent=4)
            logger.info(f"Dump config to {config_file}")


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    args = config()
    args.sleep_after_execution = 1.5
    prepare(args)

    test_file_list = []
    if args.batch_tasks_file:
        with open(args.batch_tasks_file) as f:
            batch_tasks = json.load(f)
        if args.task_ids:
            task_id_set = set(args.task_ids)
            batch_tasks = [t for t in batch_tasks if t.get("index") in task_id_set]
        batch_cfg_dir = Path(args.result_dir) / "batch_configs"
        batch_cfg_dir.mkdir(parents=True, exist_ok=True)
        for task in batch_tasks:
            task_id = task.get("index")
            if task_id is None:
                continue
            start_url = None
            for ev in task.get("evaluation", []):
                url = ev.get("content", {}).get("url")
                if url:
                    start_url = url
                    break
            config_payload = {
                "task_id": task_id,
                "intent": task.get("task", ""),
                "task": task.get("task", ""),
                "index": task_id,
                "reference_task_length": task.get("reference_task_length", 0),
                "evaluation": task.get("evaluation", []),
                "image": task.get("image", None),
                "storage_state": task.get("storage_state", None),
                "start_url": start_url,
            }
            config_path = batch_cfg_dir / f"{task_id}.json"
            with open(config_path, "w") as f:
                json.dump(config_payload, f, indent=2)
            test_file_list.append(str(config_path))
    else:
        test_config_base_dir = args.test_config_base_dir
        for test_file_id in args.task_ids:
            test_file_list.append(os.path.join(test_config_base_dir, f"{test_file_id}.json"))

    test_file_list = get_unfinished(test_file_list, args.result_dir)
    print(f"finding results at {args.result_dir}")
    print(f"Total {len(test_file_list)} tasks left")
    args.render = RENDER
    args.render_screenshot = True
    args.save_trace_enabled = True
    args.current_viewport_only = True
    dump_config(args)

    meta_test(args, test_file_list)
